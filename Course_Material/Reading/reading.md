# üìö Reading

**Cover Image Placeholder:** `https://images.unsplash.com/photo-1495640452828-3df6795cf69b?ixlib=rb-1.2.1&q=85&fm=jpg&crop=entropy&cs=srgb`

The lecture material is not self-contained; I provide supplementary material for further reading, which are in the form of [books](#reference-texts), and [course-specific reading material](#recommended-articles) (structured by week).

**Table of Contents**

---

*   [üìó Reference Texts](#reference-texts)
    *   [üîñ Chapters per Week](#chapters-per-week)
    *   [‚≠ê Optional Supplementary Material](#optional-supplementary-material)
*   [üì∞ Recommended Articles](#recommended-articles)
    *   [üìú Week 1](#week-1)
    *   [üíæ Week 2](#week-2)
    *   [üè∑Ô∏è Week 3](#week-3)
    *   [üñºÔ∏è Week 4](#week-4)
    *   [‚õèÔ∏è Week 5](#week-5)
    *   [ü§ñ Week 6](#week-6)
    *   [‚öñÔ∏è Week 7](#week-7)
        *   [‚ùó This article is part of the exam](#this-article-is-part-of-the-exam)

---

## üìó Reference Texts

---

| Our main driver for reading will be the following (online) book: |
| :--- |
| *   **[Speech and Language Processing (3rd ed. draft august 24)](https://web.stanford.edu/~jurafsky/slp3/old_aug24)** |
| Dan Jurafsky and James H. Martin |

> üí° The second edition is available as a physical purchase. It's a really good source if you're interested in NLP, but difficult to come by, does not feature an extensive DNN chapter, and we will only cover a small portion.

### üîñ Chapters per Week

---

| **Week** | **Date** | **Topic** | **Chapters** |
| :--- | :--- | :--- | :--- |
| 1 | Nov 10 | üìö Introduction | - |
| 2 | Nov 17 | üíΩ Collecting Data | [Chapter 2 (File Link: 2-2.pdf)](files/2-2.pdf) |
| 3 | Nov 24 | üè∑Ô∏è Classification | [Chapters 3 (File Link: 3-3.pdf)](files/3-3.pdf), [4 (File Link: 4-4.pdf)](files/4-4.pdf), [5 (File Link: 5-5.pdf)](files/5-5.pdf) |
| 4 | Dec 01 | üñºÔ∏è Representation | [Chapters 6 (File Link: 6-6.pdf)](files/6-6.pdf), [7 (File Link: 7-7.pdf)](files/7-7.pdf) |
| 5 | Dec 08 | ‚õèÔ∏è Information Extraction | [Chapters 17 (File Link: 8-17.pdf)](files/8-17.pdf), [20.1 (File Link: 9-20.pdf)](files/9-20.pdf) |
| 6 | Dec 15 | ü§ñ Deep Learning | [Chapter 8 (File Link: 10-8.pdf)](files/10-8.pdf) Optional: [10 (File Link: 13-10.pdf)](files/13-10.pdf), [11 (File Link: 12-11.pdf)](files/12-11.pdf), [12 (File Link: 14-12.pdf)](files/14-12.pdf) |
| 7 | Jan 05 | ü¶£ Large Language Models | [Chapter 9 (File Link: 11-9.pdf)](files/11-9.pdf), [11 (File Link: 12-11.pdf)](files/12-11.pdf) Optional: [10 (File Link: 13-10.pdf)](files/13-10.pdf), [12 (File Link: 14-12.pdf)](files/14-12.pdf) |

## ‚≠ê Optional Supplementary Material

---

I recommend the following sources for diverse further reading:

*   **[Natural Language Processing](https://raw.githubusercontent.com/jacobeisenstein/gt-nlp-class/master/notes/eisenstein-nlp-notes.pdf)**
    Jacob Eisenstein
*   **[Introduction to Information Retrieval](https://nlp.stanford.edu/IR-book/)**
    Christopher D. Manning, Prabhakar Raghavan, Hinrich Sch√ºtze
*   **[A Course in Machine Learning](http://ciml.info/dl/v0_99/ciml-v0_99-all.pdf)**
    Hal Daum√© III
*   **[A Primer on Neural Network Models for Natural Language Processing](https://u.cs.biu.ac.il/~yogo/nnlp.pdf)**
    Yoav Goldberg

All of these are freely available online (see links).

## üì∞ Recommended Articles

---

> ‚ÄºÔ∏è Please note that this section may be updated on a weekly basis. Check back regularly!

> ‚ÑπÔ∏è From certain books we only use a few pages, and some articles are not open-access. You could buy them. I'm sure that would make the publishers very happy. What they would not want you to do, is copy the DOI and paste it into services such as sci-hub to get free access to those sources. They might get mad. Anyway, where were we?

### üìú Week 1

---

**Lectures**
*   [Linguistic Fundamentals for Natural Language Processing II: 100 Essentials from Semantics and Pragmatics](https://link.springer.com/book/10.1007/978-3-031-02172-5) (by Emily Bender). Tilburg University should have access to this. If not, then, read [here](#week-2-articles-and-notes).
*   [NLP: A Primer](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html) (from Practical Natural Language Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana)

**Labs**
*   [Towards Replication in Computational Cognitive Modeling: a Machine Learning Perspective](https://link.springer.com/content/pdf/10.1007/s42113-019-00055-w.pdf) (my work), provides reproducibility context and examples of ['the bitter lesson'](http://incompleteideas.net/IncIdeas/BitterLesson.html) in the lectures.
*   [Euclidean vs. Cosine Distance](https://cmry.github.io/notes/euclidean-v-cosine) (also by me üôÇ), some practical demonstration of the different similarity metrics for text mining purposes.

### üíæ Week 2

---

**Lectures**
*   [An In-depth Analysis of the Effect of Text Normalization in Social Media](https://aclanthology.org/N15-1045.pdf) (by Baldwin and Lee).
*   [Normalization of non-standard words](https://www.sciencedirect.com/science/article/abs/pii/S088523080190169X) (by Sproat et al. 2001) ‚Äî first few sections provide some nice context (if not accessible, read [here](#week-2-articles-and-notes)).

**Labs**
*   [Word knowledge in the crowd: Measuring vocabulary size and word prevalence in a massive online experiment](https://journals.sagepub.com/doi/full/10.1080/17470218.2015.1022560) (by Keuleers et al., 2015, if not accessible, read [here](#week-2-articles-and-notes)).
*   [Subtlex-UK: A New and Improved Word Frequency Database for British English](https://journals.sagepub.com/doi/full/10.1080/17470218.2013.850521) (by van Heuven et al., 2014)

### üè∑Ô∏è Week 3

---

*   [Probabilistic models of language processing and acquisition](https://home.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/ChaterManning2006.pdf) (by Chater & Manning, 2006).
*   [Reproducibility in Machine Learning-Based Studies: An Example of Text Mining](https://openreview.net/pdf?id=By4l2PbQ-) (by Olorisade et al., 2017)

### üñºÔ∏è Week 4

---

**Lectures**
*   [Representing Spatial Structure Through Maps and Language: Lord of the Rings Encodes the Spatial Structure of Middle Earth](https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12000) (by Louwerse & Benesh, 2012).
*   [Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change](https://arxiv.org/pdf/1605.09096.pdf) (by Hamilton et al., 2018)

**Labs**
*   [Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource](https://arxiv.org/abs/1607.00225) (by Tulkens et al., 2016)
*   [Gensim for Data Science](https://learn-scikit.oneoffcoder.com/gensim.html) (Scikit, No Tears)

### ‚õèÔ∏è Week 5

---

*   [Talking sense: using machine learning to understand quotes](https://www.theguardian.com/info/2021/nov/25/talking-sense-using-machine-learning-to-understand-quotes) (spaCy applied in practice for The Guardian)
*   [Linguistically debatable or just plain wrong?](https://aclanthology.org/P14-2083.pdf) (Plank et al., 2014)

### ü§ñ Week 6

---

*   [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) (Jay Allamar, 2018 ‚Äî his blog in general is incredibly informative)
*   [Compositionality Decomposed: How do Neural Networks Generalise](https://www.jair.org/index.php/jair/article/download/11674/26576/) (Hupkes et al., 2020)

### ‚öñÔ∏è Week 7

---

**In-Person Lecture**
*   [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922) (Bender et al., 2021)
*   [The Values Encoded in Machine Learning Research](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533083) (Birhane et al., 2022)

#### ‚ùó This article is part of the exam

[The Social Impact of Natural Language Processing (ACL Anthology Bookmark Placeholder)](https://aclanthology.org/P16-2096/)
***
---